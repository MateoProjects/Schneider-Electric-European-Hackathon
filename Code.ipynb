{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSV61ZrF7vcGRI5r6NRhcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MateoProjects/Schneider-Electric-European-Hackathon/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Schneider Electric European Hackathon\n",
        "\n",
        "**Authors** : Anton ... , Ramon Mateo\n",
        "\n",
        "**Description**: To do"
      ],
      "metadata": {
        "id": "ePa7BhCRUnEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "Q7LSt-h6VIzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXUr3FuyUSxT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install keras-cv\n",
        "!cp \"poner path del csv\" .\n",
        "!cp \"path del zip con imagenes\" .\n",
        "!unzip -qq MAMe_data_256.zip\n",
        "!unzip -qq MAMe_metadata.zip\n",
        "!rm MAMe_data_256.zip\n",
        "!rm MAMe_metadata.zip\n",
        "!rm MAMe_dataset.csv\n",
        "!cp /content/drive/MyDrive/DL/L2/MAMe_dataset.csv .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "3OFpvxEuWlRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import keras_cv\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import applications\n",
        "from skimage.transform import rotate, rescale\n",
        "from keras.models import Sequential, Model "
      ],
      "metadata": {
        "id": "cdx9s8UWWnK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "18O55pvCgi0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 20"
      ],
      "metadata": {
        "id": "GWLdNi_agkiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "rMUFXnEsVhqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir_train = \"\"\n",
        "target_dir_train = \"\"\n",
        "input_dir_test = \"\"\n",
        "target_dir_test = \"\"\n"
      ],
      "metadata": {
        "id": "eKV02QS1Vjb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator"
      ],
      "metadata": {
        "id": "V7lhz7TvamAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "\n",
        "  def __init__(self, x_names, y_names, batch_size):\n",
        "    \n",
        "    self.x_values = x_names.copy()\n",
        "    self.y_values =   y_names.copy()\n",
        "    self.num_imgs = len(x_names)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # position of the batch in the sequence\n",
        "\n",
        "    aux_index = index * self.batch_size\n",
        "\n",
        "    x = [] # numpy array with shape (batch_size, input_height, input_width, input_channel)\n",
        "    y = [] # numpy array with shape (batch_size, num_classes)\n",
        "    \n",
        "\n",
        "    for _ in range(self.batch_size):\n",
        "\n",
        "      img_name = self.x_values[aux_index]\n",
        "      img = cv2.imread('/content/data_256/' + img_name)\n",
        "      x.append(img)\n",
        "      y.append(self.y_values[aux_index])\n",
        "      aux_index += 1\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x, y\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    num = random.randint(0,1000)\n",
        "    random.Random(num).shuffle(self.x_values)\n",
        "    random.Random(num).shuffle(self.y_values)\n",
        "\n",
        "  def __len__ (self):\n",
        "    # return the number of batches the generator can produce\n",
        "    return (self.num_imgs) // self.batch_size"
      ],
      "metadata": {
        "id": "LQdLJz6qan8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "A0kH25f1apbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "VGG16\n",
        "\"\"\"\n",
        "\n",
        "def get_vgg16(img_width=256, img_height=256):\n",
        "  \"\"\" \n",
        "  VGG16 for image classification \n",
        "  @return Return vgg16 with more dense layers\n",
        "  \"\"\"\n",
        "  model = applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
        "  #model.trainable = True\n",
        "  for layer in model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  #Adding custom Layers \n",
        "  x = model.output\n",
        "  x = keras.layers.Flatten()(x)\n",
        "  x = keras.layers.Dense(4096, activation=\"relu\")(x)\n",
        "  x = keras.layers.Dropout(0.2)(x)\n",
        "  x = keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "  predictions = keras.layers.Dense(NUM_LABELS, activation=\"softmax\")(x)\n",
        "  return predictions\n",
        "\n",
        "\n",
        "def get_resnet50(img_width=256, img_height=256):\n",
        "  model = applications.resnet50.Resnet50(weights=\"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
        "  return model\n",
        "\n",
        "\n",
        "models = {\"resnet50\": get_resnet50, \"vgg16\": get_vgg16}"
      ],
      "metadata": {
        "id": "hIZ7z1RsauZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Model and Hyper parameters { run: \"auto\", display-mode: \"form\" }\n",
        "model_name = \"resnet50\" #@param [\"resnet50\", \"vgg6\"]\n",
        "EPOCHS = 50 #@param {type: \"integer\"}\n",
        "BATCHSIZE = 16 #@param {type: \"integer\"}\n",
        "IMG_SIZE = 256 #@param [128,256,512, 1024]\n",
        "DATA_AUGMENTATION = True #@param {type: \"boolean\"}\n",
        "\n",
        "model = models[model_name](IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "if DATA_AUGMENTATION:\n",
        "  # concatenamos paths de nuestras imagenes con DA con las de la base\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "Pjxoel7be5r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "hSMnBQhDgGi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Push play for train { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5,verbose=0,mode=\"auto\",restore_best_weights=True)\n",
        "metric_f1 =  tfa.metrics.F1Score(num_classes, average=None, , name='f1_score', dtype: tfa.types.AcceptableDTypes = None)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy', metric_f1])\n",
        "data = DataGenerator()\n",
        "mdl_fit = model.fit(data, epochs = EPOCHS, validation_data = data_val, batch_size=BATCH_SIZE,callbacks = [early_stopping])\n"
      ],
      "metadata": {
        "id": "Tl0bmPQ9gJ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots\n"
      ],
      "metadata": {
        "id": "WIo9G8nPgJen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss\n",
        "plt.plot(mdl_fit.history['loss'], label='train loss')\n",
        "plt.plot(mdl_fit.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.savefig(\"model_\"+ model_name + \"data_epochs_\" +str(EPOCHS)+\"_batchSize_\"+ str(BATCHSIZE)+'_LossVal_loss')\n",
        "plt.show()\n",
        "# plot the accuracy\n",
        "plt.plot(mdl_fit.history['accuracy'], label='train acc')\n",
        "plt.plot(mdl_fit.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.savefig(\"model_\" + model_name +\"data_epochs_\" +str(EPOCHS)+\"_batchSize_\"+ str(BATCHSIZE)+'_AccVal_acc')\n",
        "#save model to disk\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oUr8L3SGfUfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F1 Score"
      ],
      "metadata": {
        "id": "5-LxTVuhuUG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "f1 = (2*tp)/(2*tp + fp + fn)\n",
        "\n",
        "print(\"True Positive:\" , tp)\n",
        "print(\"True Negative:\", tn)\n",
        "print(\"False positve:\", fp)\n",
        "print(\"False negative:\", fn)\n",
        "print(\"F1 Score: \", f1)"
      ],
      "metadata": {
        "id": "vZrOc-eWuTkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Note: This code was executed in other script. Code below is only for show how Data augmentation was do it. You can find the main script on the same repository"
      ],
      "metadata": {
        "id": "QBfCuAkFwBc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Data Augmentation\n",
        "\n",
        "Generating new images rotating the original image."
      ],
      "metadata": {
        "id": "998iGFdbwiqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from skimage import io, transform\n",
        "\n",
        "input_path = \"Training_Data_p1\\\\ISBI2016_ISIC_Part1_Training_Data\\\\\"\n",
        "target_path = \"Training_GroundTruth_p1\\\\ISBI2016_ISIC_Part1_Training_GroundTruth\\\\\" # esto debe ser modificado para que lea el csv o generar uno nuevo\n",
        "output_path_img = \"Training_Data_p1_DA\\\\\"\n",
        "output_path_ground = \"Training_GroundTruth_p1_DA\\\\\" # lo mismo que lo de antes. Mi idea es generar un csv nuevo con las imagenes aplicando el DA y luego se concatenan al original\n",
        "\n",
        "\n",
        "def get_paths(input_dir, target_dir):\n",
        "  input_img_paths = sorted(\n",
        "      [\n",
        "          os.path.join(input_dir, fname)\n",
        "          for fname in os.listdir(input_dir)\n",
        "          if fname.endswith(\".jpg\")\n",
        "      ]\n",
        "  )\n",
        "  target_img_paths = sorted(\n",
        "      [\n",
        "          os.path.join(target_dir, fname)\n",
        "          for fname in os.listdir(target_dir)\n",
        "          if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "      ]\n",
        "  )\n",
        "  return input_img_paths, target_img_paths\n",
        "\n",
        "def data_augmentation(input_path, target_path, output_path, output_target_path):\n",
        "    for path_in, path_targ in zip(input_path, target_path):\n",
        "        name = path_in.split(\"\\\\\")[-1].split(\".\")[0]\n",
        "        img = io.imread(path_in)\n",
        "        target = io.imread(path_targ)\n",
        "        img_90 = transform.rotate(img,90)\n",
        "        target_90 = transform.rotate(target,90)\n",
        "        img_180 = transform.rotate(img, 180)\n",
        "        target_180 = transform.rotate(target, 180)\n",
        "        io.imsave(output_path + name + \"_90.jpg\", img_90)\n",
        "        io.imsave(output_path + name + \"_180.jpg\", img_180)\n",
        "        io.imsave(output_target_path + name + \"_90.png\", target_90) # esta linea debe ser sustituida para añadir al csv el label\n",
        "        io.imsave(output_target_path + name + \"_180.png\", target_180) #lo mismo que la anterior \n",
        "\n",
        "        \n",
        "\n",
        "path_inp, path_targ = get_paths(input_path, target_path)\n",
        "data_augmentation(path_inp, path_targ, output_path_img, output_path_ground)"
      ],
      "metadata": {
        "id": "axaQFLOcwMx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation using CutMix, MixUp, and RandAugment image augmentation with KerasCV\n",
        "\n",
        "Link: https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/\n"
      ],
      "metadata": {
        "id": "EDXow03TwwP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_dataset(dataset, title):\n",
        "    plt.figure(figsize=(6, 6)).suptitle(title, fontsize=18)\n",
        "    for i, samples in enumerate(iter(dataset.take(9))):\n",
        "        images = samples[\"images\"]\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "rand_augment = keras_cv.layers.RandAugment(\n",
        "    #value_range describes the range of values covered in your images\n",
        "    value_range=(0, 255),\n",
        "    #augmentations_per_image is an integer telling the layer how many augmentations to apply to each individual image\n",
        "    augmentations_per_image=3,\n",
        "    # magnitude is a value between 0 and 1, describing the strength of the perturbations applied\n",
        "    magnitude=0.3,\n",
        "    # (Optional) magnitude_stddev allows magnitude to be randomly sampled from a distribution with a standard deviation of magnitude_stddev\n",
        "    magnitude_stddev=0.2,\n",
        "    # (Optional) rate indicates the probability to apply the augmentation applied at each layer.\n",
        "    rate=0.5,\n",
        ")\n",
        "data_ra = rand_augment(data)\n",
        "\n",
        "visualize_dataset(data, \"After rand_augment\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bTeVDxYFw-Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_mix = keras_cv.layers.CutMix()\n",
        "mix_up = keras_cv.layers.MixUp()\n",
        "\n",
        "\n",
        "def cut_mix_and_mix_up(samples):\n",
        "    samples = cut_mix(samples, training=True)\n",
        "    samples = mix_up(samples, training=True)\n",
        "    return samples\n",
        "\n",
        "data_cm = cut_mix_and_mix_up(samples)\n",
        "visualize_dataset(data_cm, \"After CutMix and MixUp\")\n"
      ],
      "metadata": {
        "id": "CzP_nbXD2FwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}